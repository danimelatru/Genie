# Mini-Genie: Latent Action World Models ğŸ§â€â™‚ï¸

A generative World Model that learns to play and simulate **MiniGrid** environments from pixels, without access to ground-truth actions.

This project implements a **VQ-VAE** to compress visual observations into discrete tokens and a **Transformer Dynamics Model** to discover latent actions and predict future states (dreams) using **Entropy Regularization**.

---

## ğŸ§  Architecture

1. **The Eyes (VQ-VAE)**  
   Compresses 64Ã—64 game frames into a 16Ã—16 grid of discrete tokens.

2. **The Brain (Transformer)**  
   Learns the physics of the world. It infers *latent actions* (e.g., Left, Right, Forward) purely by observing state transitions, clustering them via entropy regularization.

3. **The Dream (Generative Loop)**  
   Autoregressively hallucinates consistent future trajectories by feeding predictions back into the model.

---

## ğŸ“Š Results

### 1. Latent Action Discovery (t-SNE)
The model successfully separates agent behaviors into distinct action clusters in a fully unsupervised way.

![t-SNE Latent Space](assets/action_latent_space_tsne.png)

### 2. Dreaming the Future
An example of the agent â€œdreamingâ€ a trajectory by predicting 50 future frames from a single initial observation.

![Dream GIF](assets/dream_real_data.gif)

---

## ğŸš€ How to Run

### 1. Installation

```bash
conda create -n mini-genie python=3.10
conda activate mini-genie
pip install -r requirements.txt
pip install gym-minigrid
```

---

### 2. Data Collection

Record 1,000 active episodes where the agent explores the environment:

```bash
python src/record_active_data.py
```

---

### 3. Training Pipeline (End-to-End)

This script tokenizes the data, trains the Transformer dynamics model, and generates visualizations:

```bash
sbatch scripts/fix_and_train.sh
```

(Optional) Retrain the VQ-VAE (vision module) if generated dreams appear blurry or gray:

```bash
sbatch scripts/retrain_vision.sh
```

---

## ğŸ“‚ Project Structure

```text
mini-genie/
â”œâ”€â”€ data/                  # Datasets and Artifacts
â”‚   â”œâ”€â”€ episodes/          # Raw .npz game recordings (1000 files)
â”‚   â”œâ”€â”€ tokens/            # Tokenized episodes (VQ-VAE output)
â”‚   â””â”€â”€ artifacts/         # Saved models (.pth), plots, and GIFs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ record_active_data.py       # 1. Generates raw game data (MiniGrid)
â”‚   â”œâ”€â”€ train_vqvae.py              # 2. Trains the "Eyes" (Visual Compressor)
â”‚   â”œâ”€â”€ tokenize_data.py            # 3. Converts images to discrete tokens
â”‚   â”œâ”€â”€ train_transformer_dynamics.py # 4. Trains the "Brain" (World Model)
â”‚   â”œâ”€â”€ visualize_tsne.py           # 5. Analysis: Plots the Brain's latent concepts
â”‚   â””â”€â”€ generate_dream_gif.py       # 6. Visualization: Generates the dream video
â””â”€â”€ scripts/               
    â”œâ”€â”€ fix_and_train.sh            # Main pipeline (Tokenize -> Train -> Viz)
    â””â”€â”€ retrain_vision.sh           # Optional: Retrain VQ-VAE only
```

---

## ğŸ Final Steps

1. **Delete** unnecessary intermediate files to clean the workspace.
2. **Edit `src/record_active_data.py`**: set `TOTAL_EPISODES = 1000`.
3. **Edit `src/train_transformer_dynamics.py`**: set `EPOCHS = 30`.
4. **Run** data collection:
   ```bash
   python src/record_active_data.py
   ```
   (â‰ˆ10â€“20 minutes)
5. **Run** training:
   ```bash
   sbatch scripts/fix_and_train.sh
   ```
   (â‰ˆ3 hours)

Go get that 1000-episode model â€” this will be the definitive version. ğŸš€